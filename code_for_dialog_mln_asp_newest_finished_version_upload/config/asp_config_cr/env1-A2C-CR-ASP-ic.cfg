# Error model: 0% error rate, addditive confscorer, uniform nbestgenerator
# User model: standard sampled params, sampled patience
# Masks: off

###### General parameters ######
[GENERAL]
domains = CamRestaurants
singledomain = True
add_knowledge = True
tracedialog = 0
seed = 1745229

[exec_config]
configdir = _benchmarkpolicies
logfiledir = _benchmarklogs
numtrainbatches = 40
traindialogsperbatch = 100
numbatchtestdialogs =  100
trainsourceiteration = 0
numtestdialogs =  100
trainerrorrate = 0
testerrorrate  = 0
testeverybatch = True
deleteprevpolicy = True

[logging]
usecolor = False
screen_level = results
file_level = dial
file = auto

[goalgenerator]
maxvenuespergoal = 15

###### Environment parameters ######

[agent]
maxturns = 25

[usermodel]
usenewgoalscenarios = True
oldstylepatience = False
patience = 4,6
configfile = config/sampledUM_high_combination.cfg

[errormodel]
nbestsize = 1
confusionmodel = RandomConfusions
nbestgeneratormodel = SampledNBestGenerator
confscorer = additive


[summaryacts]
maxinformslots = 5
informmask = False
requestmask = False
informcountaccepted = 4
byemask = False

###### Dialogue Manager parameters ######

## Comment the following lines if using any other policy (this uses handcrafted policy)##
;[policy]
;policydir = _benchmarkpolicies
;belieftype = focus
;useconfreq = False
;learning = True
;policytype = hdc
;startwithhello = False
;inpolicyfile = auto
;outpolicyfile = auto

## Uncomment for GP policy ##
#[policy]
#policydir = _benchmarkpolicies
#belieftype = focus
#useconfreq = False
#learning = True
#policytype = gp
#startwithhello = False
#inpolicyfile = auto
#outpolicyfile = auto
#
#[gppolicy]
#kernel = polysort
#
#[gpsarsa]
#random = False
#scale = 3

# Uncomment for DQN policy ##
# [policy]
# policydir = _benchmarkpolicies
# belieftype = focus
# useconfreq = False
# learning = True
# policytype = dqn
# startwithhello = False
# inpolicyfile = auto
# outpolicyfile = auto

# [dqnpolicy]
# maxiter = 4000
# gamma = 0.99
# learning_rate = 0.001
# tau = 0.02
# replay_type = vanilla
# minibatch_size = 64
# capacity = 200000
# exploration_type = e-greedy
# episodeNum= 0.0
# epsilon_start = 0.3
# epsilon_end = 0.01
# n_in = 227
# features = ["discourseAct", "method", "requested", "full", "lastActionInformNone", "offerHappened", "inform_info"]
# max_k = 5
# learning_algorithm = dqn
# architecture = vanilla
# h1_size = 300
# h2_size = 100
# training_frequency = 2
# n_samples = 1
# stddev_var_mu = 0.01
# stddev_var_logsigma = 0.01
# mean_log_sigma = 0.000001
# sigma_prior = 1.5
# alpha =0.85
# alpha_divergence =False
# sigma_eps = 0.01
# delta = 1.0
# beta = 0.95
# is_threshold = 5.0
# train_iters_per_episode = 1

# Uncomment for A2C policy ##
# tracer
[policy]
policydir = _benchmarkpolicies
belieftype = focus
useconfreq = False
learning = True
policytype = a2c
startwithhello = False
inpolicyfile = auto
outpolicyfile = auto

[dqnpolicy]
maxiter = 4000
gamma = 0.99
# learning_rate = 0.001
# learning_rate = 0.002

critic_lr = 0.001
actor_lr = 0.0001
learning_rate = 0.005
tau = 0.02
replay_type = vanilla
minibatch_size = 64
# capacity = 1000
capacity = 6000
exploration_type = e-greedy
episodeNum= 0.0
epsilon_start = 0.6
epsilon_end = 0.0
n_in = 180
features = ["discourseAct", "method", "requested", "full", "lastActionInformNone", "offerHappened", "inform_info"]
max_k = 5
learning_algorithm = dqn
architecture = vanilla
h1_size = 250
h2_size = 130
training_frequency = 2
n_samples = 1
stddev_var_mu = 0.01
stddev_var_logsigma = 0.01
mean_log_sigma = 0.000001
sigma_prior = 1.5
alpha =0.85
alpha_divergence =False
sigma_eps = 0.01
delta = 1.0
beta = 0.95
is_threshold = 5.0
train_iters_per_episode = 1

## Uncomment for eNAC policy ##
#[policy]
#policydir = _benchmarkpolicies
#belieftype = focus
#useconfreq = False
#learning = True
#policytype = enac
#startwithhello = False
#inpolicyfile = auto
#outpolicyfile = auto

#[dqnpolicy]
#maxiter = 4000
#gamma = 0.99
#learning_rate = 0.001
#tau = 0.02
#replay_type = vanilla
#minibatch_size = 64
#capacity = 1000
#exploration_type = e-greedy
#episodeNum= 0.0
#epsilon_start = 0.3
#epsilon_end = 0.0
#n_in = 268
#features = ["discourseAct", "method", "requested", "full", "lastActionInformNone", "offerHappened", "inform_info"]
#max_k = 5
#learning_algorithm = dqn
#architecture = vanilla
#h1_size = 130
#h2_size = 50
#training_frequency = 2
#n_samples = 1
#stddev_var_mu = 0.01
#stddev_var_logsigma = 0.01
#mean_log_sigma = 0.000001
#sigma_prior = 1.5
#alpha =0.85
#alpha_divergence =False
#sigma_eps = 0.01
#delta = 1.0
#beta = 0.95
#is_threshold = 5.0
#train_iters_per_episode = 1

###### Evaluation parameters ######

[eval]
add_asp = True
asp_diff = True
inaccurate_asp = False
incomplete_asp = True
rewardvenuerecommended=0
penaliseallturns = True
wrongvenuepenalty = 0
notmentionedvaluepenalty = 0
successmeasure = objective
successreward = 20

[mln]
use_custom_inference_model = True